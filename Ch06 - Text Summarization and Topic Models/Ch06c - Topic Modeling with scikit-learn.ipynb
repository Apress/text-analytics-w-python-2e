{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nips01', 'nips04', 'MATLAB_NOTES', 'nips10', 'nips02', 'idx', 'nips11', 'nips03', 'nips07', 'README_yann', 'nips05', 'nips12', 'nips06', 'RAW_DATA_NOTES', 'orig', 'nips00', 'nips08', 'nips09']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# Read all texts into a list.\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 \n",
      "Scaling Properties of Coarse-Coded Symbol Memories \n",
      "Ronald Rosenfeld \n",
      "David S. Touretzky \n",
      "Computer Science Department \n",
      "Carnegie Mellon University \n",
      "Pittsburgh, Pennsylvania 15213 \n",
      "Abstract\n",
      "Coarse-coded symbol memories have appeared in several neural network \n",
      "symbol processing models. In order to determine how these models would scale, one \n",
      "must first have some understanding of the mathematics of coarse-coded representa- \n",
      "tions. We define the general structure of coarse-coded symbol memories and derive \n",
      "mathematical relationships among their essential parameters: memort size, slmbol-set \n",
      "size and capacitor. The computed capacity of one of the schemes agrees well with actual \n",
      "measurements of the coarse-coded working memory of DCPS, Touretzky and Hinton's \n",
      "distributed connectionist production system. \n",
      "1 Introduction \n",
      "A dstributed representation is a memory scheme in which each entity (concept, symbol) \n",
      "is represented by a pattern of activity over many units [3]. If each unit partic\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "CPU times: user 39.8 s, sys: 159 ms, total: 40 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers\n",
    "    \n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 14408)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
    "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
    "                     preprocessor=lambda doc: doc)\n",
    "cv_features = cv.fit_transform(norm_papers)\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 14408\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 25s, sys: 1min 3s, total: 16min 28s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "TOTAL_TOPICS = 20\n",
    "\n",
    "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
    "document_topics = lsi_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 14408)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_terms = lsi_model.components_\n",
    "topic_terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('state', 0.221), ('neuron', 0.169), ('image', 0.138), ('cell', 0.13), ('layer', 0.13), ('feature', 0.127), ('probability', 0.121), ('hidden', 0.114), ('distribution', 0.105), ('rate', 0.098), ('signal', 0.095), ('task', 0.093), ('class', 0.092), ('noise', 0.09), ('net', 0.089), ('recognition', 0.089), ('representation', 0.088), ('field', 0.082), ('rule', 0.082), ('step', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('cell', 0.417), ('neuron', 0.39), ('response', 0.175), ('stimulus', 0.155), ('visual', 0.131), ('spike', 0.13), ('firing', 0.117), ('synaptic', 0.11), ('activity', 0.104), ('cortex', 0.097), ('field', 0.085), ('frequency', 0.085), ('direction', 0.082), ('circuit', 0.082), ('motion', 0.082)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.289), ('probability', -0.109), ('hidden', -0.098), ('class', -0.091), ('policy', -0.081)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('state', 0.574), ('neuron', 0.212), ('action', 0.187), ('policy', 0.149), ('control', 0.12), ('dynamic', 0.1), ('cell', 0.083), ('reinforcement', 0.081), ('optimal', 0.075), ('reinforcement learning', 0.068)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.364), ('feature', -0.223), ('object', -0.144), ('recognition', -0.143), ('classifier', -0.111), ('class', -0.106), ('layer', -0.092), ('classification', -0.085), ('face', -0.073), ('test', -0.069)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.425), ('state', 0.326), ('object', 0.215), ('feature', 0.159), ('action', 0.147), ('visual', 0.143), ('control', 0.126), ('task', 0.111), ('policy', 0.103), ('recognition', 0.103), ('face', 0.092), ('representation', 0.086), ('motion', 0.086)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -0.216), ('distribution', -0.166), ('class', -0.112), ('bound', -0.109), ('probability', -0.108), ('spike', -0.104), ('variable', -0.087)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('layer', 0.261), ('net', 0.225), ('hidden', 0.222), ('neuron', 0.216), ('word', 0.206), ('recognition', 0.17), ('speech', 0.152), ('hidden unit', 0.11), ('architecture', 0.102), ('task', 0.094), ('activation', 0.092), ('memory', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.227), ('distribution', -0.222), ('image', -0.175), ('gaussian', -0.125), ('variable', -0.112), ('density', -0.108), ('probability', -0.099), ('approximation', -0.091)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('cell', 0.548), ('layer', 0.139), ('word', 0.124), ('hidden', 0.111), ('classifier', 0.097), ('direction', 0.09), ('head', 0.078), ('rule', 0.073), ('rat', 0.073), ('speech', 0.071)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -0.416), ('image', -0.336), ('circuit', -0.126), ('noise', -0.124), ('chip', -0.121), ('analog', -0.099), ('object', -0.09), ('spike', -0.075), ('signal', -0.071), ('voltage', -0.069)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.294), ('recognition', 0.252), ('speech', 0.213), ('probability', 0.194), ('classifier', 0.181), ('spike', 0.179), ('state', 0.162), ('class', 0.14), ('neuron', 0.136), ('rate', 0.123), ('hmm', 0.119), ('feature', 0.112), ('classification', 0.097), ('speaker', 0.093), ('cell', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('hidden', -0.207), ('layer', -0.179), ('hidden unit', -0.16), ('net', -0.136), ('field', -0.117)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('signal', 0.278), ('noise', 0.208), ('speech', 0.197), ('word', 0.165), ('hidden', 0.123), ('control', 0.117), ('motion', 0.116), ('filter', 0.108), ('frequency', 0.102)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('classifier', -0.225), ('node', -0.21), ('class', -0.197), ('feature', -0.186), ('neuron', -0.177), ('tree', -0.162), ('cell', -0.133), ('image', -0.119), ('rule', -0.115), ('object', -0.106), ('decision', -0.103)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('circuit', 0.244), ('control', 0.242), ('classifier', 0.229), ('chip', 0.167), ('node', 0.137), ('current', 0.132), ('analog', 0.13), ('voltage', 0.129), ('signal', 0.118), ('controller', 0.088)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('hidden', -0.27), ('neuron', -0.247), ('state', -0.175), ('distribution', -0.158), ('hidden unit', -0.143), ('layer', -0.125), ('object', -0.115), ('probability', -0.108), ('image', -0.1), ('representation', -0.098)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('circuit', 0.245), ('cell', 0.225), ('node', 0.211), ('state', 0.183), ('image', 0.166), ('chip', 0.163), ('analog', 0.147), ('layer', 0.144), ('net', 0.12), ('voltage', 0.115)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('task', -0.201), ('rule', -0.193), ('spike', -0.166), ('feature', -0.165), ('control', -0.157), ('neuron', -0.144), ('rate', -0.134), ('stimulus', -0.116), ('classifier', -0.116), ('action', -0.112)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #11:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.315), ('cell', 0.225), ('hidden', 0.205), ('spike', 0.192), ('noise', 0.163), ('rate', 0.141), ('hidden unit', 0.141), ('rule', 0.138), ('signal', 0.119), ('net', 0.111)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('field', -0.203), ('object', -0.2), ('word', -0.184), ('node', -0.161), ('motion', -0.136), ('visual', -0.134), ('neuron', -0.128), ('structure', -0.121), ('tree', -0.119), ('map', -0.107)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #12:\n",
      "==================================================\n",
      "Direction 1: [('rule', 0.581), ('representation', 0.156), ('word', 0.146), ('memory', 0.137), ('structure', 0.125), ('matrix', 0.108), ('cell', 0.086)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('classifier', -0.293), ('layer', -0.17), ('hidden', -0.16), ('motion', -0.129), ('neuron', -0.129), ('field', -0.12), ('class', -0.109), ('visual', -0.101), ('net', -0.092), ('state', -0.085), ('region', -0.084), ('hidden unit', -0.076), ('stimulus', -0.076)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #13:\n",
      "==================================================\n",
      "Direction 1: [('node', 0.396), ('tree', 0.262), ('spike', 0.226), ('stimulus', 0.208), ('signal', 0.169), ('representation', 0.147), ('motion', 0.142), ('response', 0.138), ('frequency', 0.109), ('visual', 0.1), ('rate', 0.097)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.231), ('feature', -0.157), ('neuron', -0.147), ('control', -0.13), ('matrix', -0.119), ('word', -0.114), ('recognition', -0.113), ('distance', -0.104), ('equation', -0.098)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #14:\n",
      "==================================================\n",
      "Direction 1: [('feature', 0.506), ('noise', 0.196), ('map', 0.171), ('signal', 0.133), ('classifier', 0.129), ('state', 0.124), ('memory', 0.122), ('orientation', 0.109), ('component', 0.103)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.254), ('control', -0.187), ('word', -0.162), ('recognition', -0.132), ('neuron', -0.131), ('object', -0.113), ('rate', -0.105), ('character', -0.099), ('probability', -0.096), ('bound', -0.089), ('rule', -0.086)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #15:\n",
      "==================================================\n",
      "Direction 1: [('rule', 0.365), ('classifier', 0.365), ('mixture', 0.171), ('node', 0.156), ('gaussian', 0.148), ('layer', 0.128), ('neuron', 0.114), ('field', 0.109), ('control', 0.108), ('image', 0.104), ('component', 0.098)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('bound', -0.189), ('word', -0.156), ('feature', -0.136), ('threshold', -0.135), ('object', -0.125), ('representation', -0.118), ('size', -0.117), ('task', -0.098), ('theorem', -0.097)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #16:\n",
      "==================================================\n",
      "Direction 1: [('object', 0.291), ('control', 0.206), ('mixture', 0.178), ('feature', 0.158), ('task', 0.132), ('cell', 0.13), ('variable', 0.125), ('expert', 0.117), ('current', 0.117), ('circuit', 0.115), ('tree', 0.101), ('distribution', 0.098)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -0.21), ('field', -0.172), ('rule', -0.138), ('rate', -0.121), ('motion', -0.116), ('character', -0.108), ('orientation', -0.107), ('image', -0.104)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #17:\n",
      "==================================================\n",
      "Direction 1: [('rule', 0.372), ('motion', 0.325), ('circuit', 0.193), ('direction', 0.175), ('neuron', 0.153), ('chip', 0.127), ('task', 0.123), ('visual', 0.113), ('velocity', 0.092), ('action', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('memory', -0.231), ('node', -0.215), ('control', -0.182), ('dynamic', -0.148), ('spike', -0.128), ('rate', -0.116), ('matrix', -0.108), ('noise', -0.103), ('fig', -0.097), ('cell', -0.093)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #18:\n",
      "==================================================\n",
      "Direction 1: [('object', 0.419), ('signal', 0.26), ('layer', 0.258), ('rule', 0.209), ('feature', 0.164), ('view', 0.162), ('net', 0.113), ('noise', 0.112), ('bound', 0.105), ('speech', 0.1)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('memory', -0.18), ('task', -0.161), ('representation', -0.14), ('hidden', -0.137), ('image', -0.135), ('hidden unit', -0.121), ('tree', -0.117), ('structure', -0.094), ('test', -0.093), ('word', -0.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #19:\n",
      "==================================================\n",
      "Direction 1: [('class', 0.287), ('memory', 0.275), ('classifier', 0.144), ('response', 0.139), ('sequence', 0.112), ('component', 0.11), ('stimulus', 0.101), ('region', 0.092), ('bound', 0.088)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('node', -0.292), ('feature', -0.244), ('field', -0.202), ('rate', -0.152), ('word', -0.146), ('spike', -0.139), ('map', -0.132), ('character', -0.127), ('policy', -0.108), ('tree', -0.092), ('noise', -0.088)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #20:\n",
      "==================================================\n",
      "Direction 1: [('map', 0.222), ('control', 0.2), ('region', 0.181), ('ii', 0.145), ('feature', 0.132), ('image', 0.122), ('bound', 0.11), ('orientation', 0.109), ('rule', 0.109), ('threshold', 0.094), ('class', 0.092)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('object', -0.31), ('motion', -0.252), ('direction', -0.229), ('memory', -0.223), ('classifier', -0.193), ('view', -0.136), ('matrix', -0.13), ('rate', -0.121), ('distance', -0.11)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([topic_terms[row, columns] \n",
    "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
    "                          key=lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1730</th>\n",
       "      <th>1731</th>\n",
       "      <th>1732</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1735</th>\n",
       "      <th>1736</th>\n",
       "      <th>1737</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>34.982</td>\n",
       "      <td>28.542</td>\n",
       "      <td>25.907</td>\n",
       "      <td>34.163</td>\n",
       "      <td>44.592</td>\n",
       "      <td>108.127</td>\n",
       "      <td>24.209</td>\n",
       "      <td>9.863</td>\n",
       "      <td>43.867</td>\n",
       "      <td>30.974</td>\n",
       "      <td>...</td>\n",
       "      <td>26.530</td>\n",
       "      <td>51.697</td>\n",
       "      <td>22.319</td>\n",
       "      <td>39.656</td>\n",
       "      <td>37.700</td>\n",
       "      <td>42.738</td>\n",
       "      <td>22.219</td>\n",
       "      <td>26.262</td>\n",
       "      <td>32.831</td>\n",
       "      <td>44.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>-5.852</td>\n",
       "      <td>-1.013</td>\n",
       "      <td>18.406</td>\n",
       "      <td>34.295</td>\n",
       "      <td>9.328</td>\n",
       "      <td>124.267</td>\n",
       "      <td>16.700</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>49.251</td>\n",
       "      <td>1.370</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.144</td>\n",
       "      <td>-4.074</td>\n",
       "      <td>-7.387</td>\n",
       "      <td>-17.564</td>\n",
       "      <td>-11.749</td>\n",
       "      <td>41.314</td>\n",
       "      <td>-5.562</td>\n",
       "      <td>-6.212</td>\n",
       "      <td>1.312</td>\n",
       "      <td>41.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>1.410</td>\n",
       "      <td>10.889</td>\n",
       "      <td>3.165</td>\n",
       "      <td>-5.642</td>\n",
       "      <td>10.170</td>\n",
       "      <td>20.190</td>\n",
       "      <td>11.138</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>17.976</td>\n",
       "      <td>-15.393</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.551</td>\n",
       "      <td>-51.893</td>\n",
       "      <td>-3.767</td>\n",
       "      <td>-4.917</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-5.136</td>\n",
       "      <td>-5.486</td>\n",
       "      <td>-18.248</td>\n",
       "      <td>6.881</td>\n",
       "      <td>7.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>-6.324</td>\n",
       "      <td>-4.124</td>\n",
       "      <td>2.862</td>\n",
       "      <td>12.258</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>-6.059</td>\n",
       "      <td>-6.440</td>\n",
       "      <td>-3.087</td>\n",
       "      <td>-9.035</td>\n",
       "      <td>12.518</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.464</td>\n",
       "      <td>49.427</td>\n",
       "      <td>-6.691</td>\n",
       "      <td>-20.647</td>\n",
       "      <td>-14.674</td>\n",
       "      <td>11.011</td>\n",
       "      <td>-2.055</td>\n",
       "      <td>1.509</td>\n",
       "      <td>2.634</td>\n",
       "      <td>-12.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>3.741</td>\n",
       "      <td>2.986</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-6.996</td>\n",
       "      <td>-7.441</td>\n",
       "      <td>-28.592</td>\n",
       "      <td>11.726</td>\n",
       "      <td>3.455</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-12.691</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-21.342</td>\n",
       "      <td>-7.650</td>\n",
       "      <td>-26.803</td>\n",
       "      <td>-16.937</td>\n",
       "      <td>-20.949</td>\n",
       "      <td>-1.934</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>-19.416</td>\n",
       "      <td>1.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>-3.479</td>\n",
       "      <td>-8.386</td>\n",
       "      <td>2.634</td>\n",
       "      <td>20.382</td>\n",
       "      <td>20.870</td>\n",
       "      <td>124.431</td>\n",
       "      <td>-14.895</td>\n",
       "      <td>0.672</td>\n",
       "      <td>7.873</td>\n",
       "      <td>-13.965</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976</td>\n",
       "      <td>-33.519</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-4.786</td>\n",
       "      <td>-6.352</td>\n",
       "      <td>16.011</td>\n",
       "      <td>-2.638</td>\n",
       "      <td>1.392</td>\n",
       "      <td>15.578</td>\n",
       "      <td>-18.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>-9.397</td>\n",
       "      <td>-6.295</td>\n",
       "      <td>-3.280</td>\n",
       "      <td>-11.858</td>\n",
       "      <td>-1.963</td>\n",
       "      <td>11.538</td>\n",
       "      <td>-1.289</td>\n",
       "      <td>-3.861</td>\n",
       "      <td>6.792</td>\n",
       "      <td>-5.678</td>\n",
       "      <td>...</td>\n",
       "      <td>12.962</td>\n",
       "      <td>14.565</td>\n",
       "      <td>-2.714</td>\n",
       "      <td>1.883</td>\n",
       "      <td>5.691</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>1.308</td>\n",
       "      <td>2.477</td>\n",
       "      <td>-1.715</td>\n",
       "      <td>29.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T8</th>\n",
       "      <td>-14.575</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-1.043</td>\n",
       "      <td>-1.868</td>\n",
       "      <td>-19.290</td>\n",
       "      <td>-35.256</td>\n",
       "      <td>-6.677</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-14.835</td>\n",
       "      <td>3.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.531</td>\n",
       "      <td>-11.503</td>\n",
       "      <td>-4.121</td>\n",
       "      <td>22.629</td>\n",
       "      <td>28.148</td>\n",
       "      <td>-17.673</td>\n",
       "      <td>5.222</td>\n",
       "      <td>-11.032</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>32.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T9</th>\n",
       "      <td>-1.051</td>\n",
       "      <td>-3.557</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-6.498</td>\n",
       "      <td>-2.187</td>\n",
       "      <td>-6.040</td>\n",
       "      <td>-3.979</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-6.426</td>\n",
       "      <td>2.538</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861</td>\n",
       "      <td>-9.045</td>\n",
       "      <td>7.977</td>\n",
       "      <td>-9.376</td>\n",
       "      <td>-6.113</td>\n",
       "      <td>-5.274</td>\n",
       "      <td>2.863</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>22.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>1.758</td>\n",
       "      <td>1.996</td>\n",
       "      <td>-8.663</td>\n",
       "      <td>10.108</td>\n",
       "      <td>27.037</td>\n",
       "      <td>28.986</td>\n",
       "      <td>-4.839</td>\n",
       "      <td>1.068</td>\n",
       "      <td>-4.521</td>\n",
       "      <td>4.474</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.462</td>\n",
       "      <td>4.377</td>\n",
       "      <td>-5.835</td>\n",
       "      <td>-3.005</td>\n",
       "      <td>5.621</td>\n",
       "      <td>7.271</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-15.318</td>\n",
       "      <td>4.018</td>\n",
       "      <td>-1.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>-13.387</td>\n",
       "      <td>-2.227</td>\n",
       "      <td>-6.642</td>\n",
       "      <td>-15.198</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>52.437</td>\n",
       "      <td>-8.159</td>\n",
       "      <td>1.714</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>9.864</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.341</td>\n",
       "      <td>30.282</td>\n",
       "      <td>-1.427</td>\n",
       "      <td>3.788</td>\n",
       "      <td>4.654</td>\n",
       "      <td>4.525</td>\n",
       "      <td>-5.705</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-2.840</td>\n",
       "      <td>28.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>18.972</td>\n",
       "      <td>3.014</td>\n",
       "      <td>-2.120</td>\n",
       "      <td>-8.130</td>\n",
       "      <td>3.860</td>\n",
       "      <td>44.542</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.930</td>\n",
       "      <td>...</td>\n",
       "      <td>7.018</td>\n",
       "      <td>11.140</td>\n",
       "      <td>3.283</td>\n",
       "      <td>-2.106</td>\n",
       "      <td>15.821</td>\n",
       "      <td>-13.837</td>\n",
       "      <td>13.028</td>\n",
       "      <td>2.523</td>\n",
       "      <td>1.869</td>\n",
       "      <td>2.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>0.862</td>\n",
       "      <td>-9.514</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>5.150</td>\n",
       "      <td>9.025</td>\n",
       "      <td>-20.882</td>\n",
       "      <td>-5.257</td>\n",
       "      <td>-1.425</td>\n",
       "      <td>-10.169</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.106</td>\n",
       "      <td>5.355</td>\n",
       "      <td>-8.155</td>\n",
       "      <td>-3.509</td>\n",
       "      <td>5.128</td>\n",
       "      <td>-8.875</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>-9.501</td>\n",
       "      <td>-7.422</td>\n",
       "      <td>39.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>11.769</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-1.705</td>\n",
       "      <td>-3.632</td>\n",
       "      <td>-15.151</td>\n",
       "      <td>-3.593</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-7.154</td>\n",
       "      <td>-5.271</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>-8.836</td>\n",
       "      <td>0.182</td>\n",
       "      <td>8.459</td>\n",
       "      <td>12.415</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>2.790</td>\n",
       "      <td>31.555</td>\n",
       "      <td>3.721</td>\n",
       "      <td>10.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T15</th>\n",
       "      <td>-17.419</td>\n",
       "      <td>5.586</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>3.847</td>\n",
       "      <td>4.443</td>\n",
       "      <td>-5.549</td>\n",
       "      <td>5.545</td>\n",
       "      <td>-1.490</td>\n",
       "      <td>0.571</td>\n",
       "      <td>2.809</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.405</td>\n",
       "      <td>12.439</td>\n",
       "      <td>-8.037</td>\n",
       "      <td>8.060</td>\n",
       "      <td>15.974</td>\n",
       "      <td>-5.808</td>\n",
       "      <td>-4.523</td>\n",
       "      <td>-14.412</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-13.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T16</th>\n",
       "      <td>-12.213</td>\n",
       "      <td>-6.177</td>\n",
       "      <td>1.563</td>\n",
       "      <td>-6.341</td>\n",
       "      <td>2.285</td>\n",
       "      <td>27.876</td>\n",
       "      <td>4.732</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>9.721</td>\n",
       "      <td>-10.141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-11.936</td>\n",
       "      <td>-6.757</td>\n",
       "      <td>11.262</td>\n",
       "      <td>2.770</td>\n",
       "      <td>-25.380</td>\n",
       "      <td>-2.386</td>\n",
       "      <td>12.245</td>\n",
       "      <td>7.274</td>\n",
       "      <td>0.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T17</th>\n",
       "      <td>-18.585</td>\n",
       "      <td>-2.763</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>10.605</td>\n",
       "      <td>-16.839</td>\n",
       "      <td>-14.887</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-3.819</td>\n",
       "      <td>-4.649</td>\n",
       "      <td>...</td>\n",
       "      <td>4.494</td>\n",
       "      <td>-10.054</td>\n",
       "      <td>4.302</td>\n",
       "      <td>4.390</td>\n",
       "      <td>-1.295</td>\n",
       "      <td>-13.236</td>\n",
       "      <td>-1.678</td>\n",
       "      <td>3.280</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-13.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T18</th>\n",
       "      <td>-12.848</td>\n",
       "      <td>7.486</td>\n",
       "      <td>-8.335</td>\n",
       "      <td>0.493</td>\n",
       "      <td>5.079</td>\n",
       "      <td>15.669</td>\n",
       "      <td>-6.052</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-11.230</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.699</td>\n",
       "      <td>-24.194</td>\n",
       "      <td>2.532</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>12.251</td>\n",
       "      <td>-8.193</td>\n",
       "      <td>-2.601</td>\n",
       "      <td>5.075</td>\n",
       "      <td>-2.323</td>\n",
       "      <td>11.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T19</th>\n",
       "      <td>18.885</td>\n",
       "      <td>3.559</td>\n",
       "      <td>4.630</td>\n",
       "      <td>-2.531</td>\n",
       "      <td>-11.831</td>\n",
       "      <td>20.836</td>\n",
       "      <td>3.319</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>5.287</td>\n",
       "      <td>-1.905</td>\n",
       "      <td>...</td>\n",
       "      <td>7.212</td>\n",
       "      <td>22.311</td>\n",
       "      <td>-1.514</td>\n",
       "      <td>-5.862</td>\n",
       "      <td>3.249</td>\n",
       "      <td>18.530</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-16.380</td>\n",
       "      <td>-7.774</td>\n",
       "      <td>-2.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T20</th>\n",
       "      <td>-16.822</td>\n",
       "      <td>-1.366</td>\n",
       "      <td>6.807</td>\n",
       "      <td>-8.222</td>\n",
       "      <td>3.332</td>\n",
       "      <td>-1.283</td>\n",
       "      <td>-3.011</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>2.718</td>\n",
       "      <td>0.907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-4.415</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>55.384</td>\n",
       "      <td>-7.929</td>\n",
       "      <td>6.537</td>\n",
       "      <td>5.476</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 1740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4        5       6      7       8     \\\n",
       "T1   34.982  28.542  25.907  34.163  44.592  108.127  24.209  9.863  43.867   \n",
       "T2   -5.852  -1.013  18.406  34.295   9.328  124.267  16.700 -1.871  49.251   \n",
       "T3    1.410  10.889   3.165  -5.642  10.170   20.190  11.138 -1.272  17.976   \n",
       "T4   -6.324  -4.124   2.862  12.258  -0.979   -6.059  -6.440 -3.087  -9.035   \n",
       "T5    3.741   2.986   0.767  -6.996  -7.441  -28.592  11.726  3.455   0.471   \n",
       "T6   -3.479  -8.386   2.634  20.382  20.870  124.431 -14.895  0.672   7.873   \n",
       "T7   -9.397  -6.295  -3.280 -11.858  -1.963   11.538  -1.289 -3.861   6.792   \n",
       "T8  -14.575  -3.000  -1.043  -1.868 -19.290  -35.256  -6.677  0.189 -14.835   \n",
       "T9   -1.051  -3.557   0.694  -6.498  -2.187   -6.040  -3.979  0.098  -6.426   \n",
       "T10   1.758   1.996  -8.663  10.108  27.037   28.986  -4.839  1.068  -4.521   \n",
       "T11 -13.387  -2.227  -6.642 -15.198  -0.026   52.437  -8.159  1.714  -0.297   \n",
       "T12  18.972   3.014  -2.120  -8.130   3.860   44.542  -0.532 -0.473   0.320   \n",
       "T13   0.862  -9.514  -0.157   5.150   9.025  -20.882  -5.257 -1.425 -10.169   \n",
       "T14  11.769  -0.914   0.316  -1.705  -3.632  -15.151  -3.593 -0.770  -7.154   \n",
       "T15 -17.419   5.586  -0.581   3.847   4.443   -5.549   5.545 -1.490   0.571   \n",
       "T16 -12.213  -6.177   1.563  -6.341   2.285   27.876   4.732 -0.210   9.721   \n",
       "T17 -18.585  -2.763  -0.496  10.605 -16.839  -14.887  -0.128 -0.182  -3.819   \n",
       "T18 -12.848   7.486  -8.335   0.493   5.079   15.669  -6.052 -0.120 -11.230   \n",
       "T19  18.885   3.559   4.630  -2.531 -11.831   20.836   3.319 -0.557   5.287   \n",
       "T20 -16.822  -1.366   6.807  -8.222   3.332   -1.283  -3.011 -0.399   2.718   \n",
       "\n",
       "       9      ...      1730    1731    1732    1733    1734    1735    1736  \\\n",
       "T1   30.974   ...    26.530  51.697  22.319  39.656  37.700  42.738  22.219   \n",
       "T2    1.370   ...   -13.144  -4.074  -7.387 -17.564 -11.749  41.314  -5.562   \n",
       "T3  -15.393   ...    -8.551 -51.893  -3.767  -4.917  -0.491  -5.136  -5.486   \n",
       "T4   12.518   ...    -9.464  49.427  -6.691 -20.647 -14.674  11.011  -2.055   \n",
       "T5  -12.691   ...    -5.500 -21.342  -7.650 -26.803 -16.937 -20.949  -1.934   \n",
       "T6  -13.965   ...     3.976 -33.519  -0.643  -4.786  -6.352  16.011  -2.638   \n",
       "T7   -5.678   ...    12.962  14.565  -2.714   1.883   5.691  -0.636   1.308   \n",
       "T8    3.065   ...    -1.531 -11.503  -4.121  22.629  28.148 -17.673   5.222   \n",
       "T9    2.538   ...     1.861  -9.045   7.977  -9.376  -6.113  -5.274   2.863   \n",
       "T10   4.474   ...    -2.462   4.377  -5.835  -3.005   5.621   7.271  -0.363   \n",
       "T11   9.864   ...    -8.341  30.282  -1.427   3.788   4.654   4.525  -5.705   \n",
       "T12   0.930   ...     7.018  11.140   3.283  -2.106  15.821 -13.837  13.028   \n",
       "T13  -0.960   ...    -5.106   5.355  -8.155  -3.509   5.128  -8.875  -5.253   \n",
       "T14  -5.271   ...    -1.385  -8.836   0.182   8.459  12.415  -0.169   2.790   \n",
       "T15   2.809   ...    -4.405  12.439  -8.037   8.060  15.974  -5.808  -4.523   \n",
       "T16 -10.141   ...    -0.902 -11.936  -6.757  11.262   2.770 -25.380  -2.386   \n",
       "T17  -4.649   ...     4.494 -10.054   4.302   4.390  -1.295 -13.236  -1.678   \n",
       "T18  -0.579   ...    -3.699 -24.194   2.532  -0.177  12.251  -8.193  -2.601   \n",
       "T19  -1.905   ...     7.212  22.311  -1.514  -5.862   3.249  18.530   0.712   \n",
       "T20   0.907   ...    -0.531  -0.725  -4.415   0.537  -0.529  55.384  -7.929   \n",
       "\n",
       "       1737    1738    1739  \n",
       "T1   26.262  32.831  44.796  \n",
       "T2   -6.212   1.312  41.708  \n",
       "T3  -18.248   6.881   7.501  \n",
       "T4    1.509   2.634 -12.566  \n",
       "T5   -0.972 -19.416   1.184  \n",
       "T6    1.392  15.578 -18.402  \n",
       "T7    2.477  -1.715  29.722  \n",
       "T8  -11.032  -0.818  32.775  \n",
       "T9   -1.993  -1.686  22.510  \n",
       "T10 -15.318   4.018  -1.234  \n",
       "T11   0.127  -2.840  28.388  \n",
       "T12   2.523   1.869   2.264  \n",
       "T13  -9.501  -7.422  39.037  \n",
       "T14  31.555   3.721  10.198  \n",
       "T15 -14.412   0.235 -13.658  \n",
       "T16  12.245   7.274   0.568  \n",
       "T17   3.280  -0.878 -13.707  \n",
       "T18   5.075  -2.323  11.345  \n",
       "T19 -16.380  -7.774  -2.067  \n",
       "T20   6.537   5.476   0.848  \n",
       "\n",
       "[20 rows x 1740 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_df = pd.DataFrame(np.round(document_topics, 3), \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T1', 'T6', 'T4']\n",
      "Paper Summary:\n",
      "9 \n",
      "Stochastic Learning Networks and their Electronic Implementation \n",
      "Joshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana \n",
      "Bell Communications Research, Morristown, NJ 07960 \n",
      "ABSTRACT\n",
      "We describe a family of learning algorithms that operate on a recurrent, symmetrically \n",
      "connected, neuromorphic network that, like the Boltzmann machine, settles in the \n",
      "presence of noise. These networks learn by modifying synaptic connection strengths on \n",
      "the basis of correlations seen loca\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T3', 'T18', 'T4']\n",
      "Paper Summary:\n",
      "266 Zemel, Mozer and Hinton \n",
      "TRAFFIC: Recognizing Objects Using \n",
      "Hierarchical Reference Frame Transformations \n",
      "Richard S. Zemel \n",
      "Computer Science Dept. \n",
      "University of Toronto \n",
      "Toronto, ONT M5S 1A4 \n",
      "Michael C. Mozer \n",
      "Computer Science Dept. \n",
      "University of Colorado \n",
      "Boulder, CO 80309-0430 \n",
      "Geoffrey E. Hinton \n",
      "Computer Science Dept. \n",
      "University of Toronto \n",
      "Toronto, ONT M5S 1A4 \n",
      "ABSTRACT \n",
      "We describe a model that can recognize two-dimensional shapes in \n",
      "an unsegmented image, independent of their orie\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T9', 'T1', 'T10']\n",
      "Paper Summary:\n",
      "Constrained Optimization Applied to the \n",
      "Parameter Setting Problem for Analog Circuits \n",
      "David Kirk, Kurt Fleischer, Lloyd Watts Alan Bart \n",
      "Computer Graphics 350-74 \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "Abstract \n",
      "We use constrained optimization to select operating parameters for two \n",
      "circuits: a simple 3-transistor square root circuit, and an analog VLSI \n",
      "artificial cochlea. This automated method uses computer controlled mea- \n",
      "surement and test equipment to choose chip paramet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 14s, sys: 1min 41s, total: 14min 56s\n",
      "Wall time: 55min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50,\n",
    "                                      learning_method='online', batch_size=1740, learning_offset=50., \n",
    "                                      random_state=42, n_jobs=16)\n",
    "document_topics = lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, principal component</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                              Terms per Topic\n",
       "Topic1   neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig                                              \n",
       "Topic2   image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line                                                  \n",
       "Topic3   neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition                                \n",
       "Topic4   cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map                                              \n",
       "Topic5   image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule                                \n",
       "Topic6   state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation                                     \n",
       "Topic7   sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio                                     \n",
       "Topic8   memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone                      \n",
       "Topic9   activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self                             \n",
       "Topic10  state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field                                         \n",
       "Topic11  generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, principal component\n",
       "Topic12  feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition                                                \n",
       "Topic13  image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map                                                               \n",
       "Topic14  neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction                                           \n",
       "Topic15  distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density                                 \n",
       "Topic16  state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high                                                  \n",
       "Topic17  state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class                                       \n",
       "Topic18  state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm                  \n",
       "Topic19  control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal                                        \n",
       "Topic20  mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq                         "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1730</th>\n",
       "      <th>1731</th>\n",
       "      <th>1732</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1735</th>\n",
       "      <th>1736</th>\n",
       "      <th>1737</th>\n",
       "      <th>1738</th>\n",
       "      <th>1739</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>0.227</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>0.446</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T8</th>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T15</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T16</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T17</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T18</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T19</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T20</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 1740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9  ...   1730  \\\n",
       "T1  0.011 0.137 0.017 0.000 0.219 0.034 0.477 0.218 0.120 0.063  ...  0.000   \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T4  0.028 0.000 0.663 0.773 0.262 0.802 0.030 0.000 0.786 0.269  ...  0.000   \n",
       "T5  0.227 0.035 0.045 0.212 0.024 0.086 0.086 0.370 0.089 0.402  ...  0.210   \n",
       "T6  0.446 0.579 0.000 0.000 0.238 0.022 0.102 0.022 0.000 0.036  ...  0.000   \n",
       "T7  0.000 0.000 0.026 0.015 0.008 0.000 0.000 0.000 0.005 0.000  ...  0.000   \n",
       "T8  0.062 0.000 0.000 0.000 0.000 0.017 0.041 0.013 0.000 0.000  ...  0.040   \n",
       "T9  0.000 0.025 0.113 0.000 0.000 0.021 0.116 0.000 0.000 0.000  ...  0.003   \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T11 0.037 0.002 0.026 0.000 0.000 0.000 0.000 0.042 0.000 0.000  ...  0.000   \n",
       "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T15 0.183 0.222 0.000 0.000 0.191 0.004 0.000 0.333 0.000 0.225  ...  0.710   \n",
       "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T18 0.000 0.000 0.109 0.000 0.057 0.008 0.146 0.000 0.000 0.004  ...  0.019   \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
       "T20 0.006 0.000 0.000 0.000 0.000 0.004 0.000 0.000 0.000 0.000  ...  0.018   \n",
       "\n",
       "     1731  1732  1733  1734  1735  1736  1737  1738  1739  \n",
       "T1  0.000 0.000 0.000 0.000 0.000 0.115 0.000 0.000 0.482  \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T4  0.107 0.000 0.000 0.000 0.999 0.000 0.000 0.130 0.518  \n",
       "T5  0.591 0.043 0.007 0.019 0.000 0.431 0.466 0.000 0.000  \n",
       "T6  0.094 0.000 0.000 0.330 0.000 0.162 0.000 0.000 0.000  \n",
       "T7  0.000 0.000 0.003 0.002 0.000 0.000 0.000 0.003 0.000  \n",
       "T8  0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000  \n",
       "T9  0.000 0.000 0.000 0.000 0.000 0.015 0.000 0.000 0.000  \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T11 0.170 0.000 0.011 0.000 0.000 0.000 0.074 0.000 0.000  \n",
       "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T15 0.028 0.937 0.907 0.597 0.000 0.238 0.446 0.403 0.000  \n",
       "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T18 0.000 0.019 0.037 0.000 0.000 0.014 0.000 0.433 0.000  \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T20 0.009 0.000 0.034 0.051 0.000 0.010 0.014 0.030 0.000  \n",
       "\n",
       "[20 rows x 1740 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics, \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Paper Num</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Paper Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>T1</td>\n",
       "      <td>0.99938</td>\n",
       "      <td>1122</td>\n",
       "      <td>neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig</td>\n",
       "      <td>Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>T2</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>T3</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>T4</td>\n",
       "      <td>0.99947</td>\n",
       "      <td>1735</td>\n",
       "      <td>cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map</td>\n",
       "      <td>Can V1 mechanisms account for \\nfigure-ground and medial axis effects? \\nZhaoping Li \\nGatsby Computational Neuroscience Unit \\nUniversity College London \\nzhaopinggat shy. ucl. ac. uk \\nAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>T5</td>\n",
       "      <td>0.99949</td>\n",
       "      <td>177</td>\n",
       "      <td>image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule</td>\n",
       "      <td>215 \\nConsonant Recognition by Modular Construction of \\nLarge Phonemic Time-Delay Neural Networks \\nAlex Waibel \\nCarnegie-Mellon University \\nPittsburgh, PA 15213, \\nATR Interpreting Telephony R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>T6</td>\n",
       "      <td>0.99684</td>\n",
       "      <td>1128</td>\n",
       "      <td>state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation</td>\n",
       "      <td>Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>T7</td>\n",
       "      <td>0.99956</td>\n",
       "      <td>283</td>\n",
       "      <td>sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio</td>\n",
       "      <td>A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>T8</td>\n",
       "      <td>0.98167</td>\n",
       "      <td>892</td>\n",
       "      <td>memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone</td>\n",
       "      <td>A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>T9</td>\n",
       "      <td>0.99929</td>\n",
       "      <td>227</td>\n",
       "      <td>activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self</td>\n",
       "      <td>44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>T10</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>T11</td>\n",
       "      <td>0.99917</td>\n",
       "      <td>1396</td>\n",
       "      <td>generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, pri...</td>\n",
       "      <td>I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>T12</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>T13</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>T14</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>T15</td>\n",
       "      <td>0.99947</td>\n",
       "      <td>609</td>\n",
       "      <td>distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density</td>\n",
       "      <td>On the Use of Evidence in Neural Networks \\nDavid H. Wolpert \\nThe Santa Fe Institute \\n1660 Old Pecos Trail \\nSanta Fe, NM 87501 \\nAbstract \\nThe Bayesian \"evidence\" approximation has recently be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>T16</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>T17</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>T18</td>\n",
       "      <td>0.99929</td>\n",
       "      <td>940</td>\n",
       "      <td>state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
       "      <td>An Actor/Critic Algorithm that is \\nEquivalent to Q-Learning \\nRobert H. Crites \\nComputer Science Department \\nUniversity of Massachusetts \\nAmherst, MA 01003 \\ncrit escs .umass. edu \\nAndrew G....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>T19</td>\n",
       "      <td>0.00033</td>\n",
       "      <td>151</td>\n",
       "      <td>control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal</td>\n",
       "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>T20</td>\n",
       "      <td>0.99939</td>\n",
       "      <td>1089</td>\n",
       "      <td>mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq</td>\n",
       "      <td>A Unified Learning Scheme: \\nBayesian-Kullback Ying-Yang Machine \\nLei Xu \\n1. Computer Science Dept., The Chinese University of HK, Hong Kong \\n2. National Machine Perception Lab, Peking Universi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topic  Contribution %  Paper Num  \\\n",
       "Topic1              T1         0.99938       1122   \n",
       "Topic2              T2         0.00033        151   \n",
       "Topic3              T3         0.00033        151   \n",
       "Topic4              T4         0.99947       1735   \n",
       "Topic5              T5         0.99949        177   \n",
       "Topic6              T6         0.99684       1128   \n",
       "Topic7              T7         0.99956        283   \n",
       "Topic8              T8         0.98167        892   \n",
       "Topic9              T9         0.99929        227   \n",
       "Topic10            T10         0.00033        151   \n",
       "Topic11            T11         0.99917       1396   \n",
       "Topic12            T12         0.00033        151   \n",
       "Topic13            T13         0.00033        151   \n",
       "Topic14            T14         0.00033        151   \n",
       "Topic15            T15         0.99947        609   \n",
       "Topic16            T16         0.00033        151   \n",
       "Topic17            T17         0.00033        151   \n",
       "Topic18            T18         0.99929        940   \n",
       "Topic19            T19         0.00033        151   \n",
       "Topic20            T20         0.99939       1089   \n",
       "\n",
       "                                                                                                                                                                                                           Topic  \\\n",
       "Topic1                                    neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig   \n",
       "Topic2                                        image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line   \n",
       "Topic3                      neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition   \n",
       "Topic4                                    cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map   \n",
       "Topic5                      image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule   \n",
       "Topic6                           state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation   \n",
       "Topic7                           sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio   \n",
       "Topic8            memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone   \n",
       "Topic9                   activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self   \n",
       "Topic10                              state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field   \n",
       "Topic11  generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, pri...   \n",
       "Topic12                                     feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition   \n",
       "Topic13                                                    image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map   \n",
       "Topic14                                neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction   \n",
       "Topic15                      distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density   \n",
       "Topic16                                       state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high   \n",
       "Topic17                            state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class   \n",
       "Topic18       state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm   \n",
       "Topic19                             control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal   \n",
       "Topic20              mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq   \n",
       "\n",
       "                                                                                                                                                                                                      Paper Name  \n",
       "Topic1   Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...  \n",
       "Topic2   794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic3   794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic4   Can V1 mechanisms account for \\nfigure-ground and medial axis effects? \\nZhaoping Li \\nGatsby Computational Neuroscience Unit \\nUniversity College London \\nzhaopinggat shy. ucl. ac. uk \\nAbstract...  \n",
       "Topic5   215 \\nConsonant Recognition by Modular Construction of \\nLarge Phonemic Time-Delay Neural Networks \\nAlex Waibel \\nCarnegie-Mellon University \\nPittsburgh, PA 15213, \\nATR Interpreting Telephony R...  \n",
       "Topic6   Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...  \n",
       "Topic7   A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...  \n",
       "Topic8   A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...  \n",
       "Topic9   44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...  \n",
       "Topic10  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic11  I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...  \n",
       "Topic12  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic13  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic14  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic15  On the Use of Evidence in Neural Networks \\nDavid H. Wolpert \\nThe Santa Fe Institute \\n1660 Old Pecos Trail \\nSanta Fe, NM 87501 \\nAbstract \\nThe Bayesian \"evidence\" approximation has recently be...  \n",
       "Topic16  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic17  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic18  An Actor/Critic Algorithm that is \\nEquivalent to Q-Learning \\nRobert H. Crites \\nComputer Science Department \\nUniversity of Massachusetts \\nAmherst, MA 01003 \\ncrit escs .umass. edu \\nAndrew G....  \n",
       "Topic19  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
       "Topic20  A Unified Learning Scheme: \\nBayesian-Kullback Ying-Yang Machine \\nLei Xu \\n1. Computer Science Dept., The Chinese University of HK, Hong Kong \\n2. National Machine Perception Lab, Peking Universi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_contrib_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_contrib_topics.index\n",
    "contrib_perc = max_contrib_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
    "                       for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 39s, sys: 47.5 s, total: 12min 26s\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
    "                random_state=42, alpha=.1, l1_ratio=.85)\n",
    "document_topics = nmf_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                     Terms per Topic\n",
       "Topic1   bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum                                            \n",
       "Topic2   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing                                \n",
       "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision                 \n",
       "Topic4   image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface                                                 \n",
       "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test\n",
       "Topic6   cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property                                                        \n",
       "Topic7   word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state                               \n",
       "Topic8   signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig                                                        \n",
       "Topic9   control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant                                             \n",
       "Topic10  circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp                                                   \n",
       "Topic11  spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution                                \n",
       "Topic12  rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned                                         \n",
       "Topic13  node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation                                        \n",
       "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra                    \n",
       "Topic15  classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian                       \n",
       "Topic16  distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum                     \n",
       "Topic17  motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed                                                       \n",
       "Topic18  object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint                                      \n",
       "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation                    \n",
       "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol                    "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_terms = nmf_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>T19</th>\n",
       "      <th>T20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.394</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.167</td>\n",
       "      <td>1.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.679</td>\n",
       "      <td>7.510</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.326</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.923</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.147</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.084</td>\n",
       "      <td>1.760</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.258</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     T1    T2    T3    T4    T5    T6    T7    T8    T9   T10   T11   T12  \\\n",
       "0 0.444 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1 0.394 0.595 0.463 0.019 0.187 0.037 0.000 0.228 0.130 0.029 0.000 0.254   \n",
       "2 0.032 0.619 0.003 0.067 0.016 0.378 0.029 0.027 0.448 0.000 0.075 0.036   \n",
       "3 0.000 0.274 0.000 0.102 0.265 1.019 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4 0.060 0.188 0.682 0.257 0.167 1.402 0.000 0.093 0.000 0.001 0.000 0.020   \n",
       "5 0.000 0.383 0.000 0.000 0.679 7.510 0.016 0.000 0.000 0.326 1.146 1.923   \n",
       "6 0.000 1.415 0.020 0.000 0.046 0.044 0.000 0.114 0.333 0.040 0.000 0.032   \n",
       "7 0.147 0.029 0.000 0.000 0.274 0.008 0.042 0.000 0.045 0.080 0.008 0.025   \n",
       "8 0.084 1.760 0.013 0.012 0.000 1.592 0.000 0.000 0.257 0.068 0.273 0.055   \n",
       "9 0.395 0.000 0.040 1.258 0.127 0.000 0.000 0.370 0.075 0.076 0.000 0.042   \n",
       "\n",
       "    T13   T14   T15   T16   T17   T18   T19   T20  \n",
       "0 0.004 0.263 0.000 0.000 0.000 0.000 0.000 3.437  \n",
       "1 0.000 0.000 0.000 0.000 0.000 0.106 0.000 0.210  \n",
       "2 0.024 0.184 0.100 0.000 0.126 0.000 0.656 0.277  \n",
       "3 0.218 0.011 0.000 0.004 1.299 0.291 1.268 0.295  \n",
       "4 1.749 0.037 0.000 0.344 0.000 0.000 0.164 0.121  \n",
       "5 0.098 0.000 0.000 0.202 0.000 0.426 0.646 0.641  \n",
       "6 0.124 0.000 0.041 0.041 0.075 0.030 0.000 0.615  \n",
       "7 0.022 0.009 0.000 0.023 0.000 0.007 0.000 0.096  \n",
       "8 0.122 0.000 0.119 0.000 0.000 0.027 0.514 0.353  \n",
       "9 0.000 0.017 0.000 0.053 0.041 0.133 0.427 0.000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics, \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Paper Num</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Paper Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>T1</td>\n",
       "      <td>1.64138</td>\n",
       "      <td>991</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&amp;T Research \\nABSTRACT\\n1 INTRODUCTION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>T2</td>\n",
       "      <td>3.58149</td>\n",
       "      <td>383</td>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "      <td>Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>T3</td>\n",
       "      <td>5.83072</td>\n",
       "      <td>1167</td>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
       "      <td>Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>T4</td>\n",
       "      <td>3.93349</td>\n",
       "      <td>1731</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>T5</td>\n",
       "      <td>2.98750</td>\n",
       "      <td>33</td>\n",
       "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...</td>\n",
       "      <td>5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>T6</td>\n",
       "      <td>7.51003</td>\n",
       "      <td>5</td>\n",
       "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
       "      <td>317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>T7</td>\n",
       "      <td>4.89525</td>\n",
       "      <td>1318</td>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "      <td>Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>T8</td>\n",
       "      <td>3.67982</td>\n",
       "      <td>235</td>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "      <td>232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>T9</td>\n",
       "      <td>4.88831</td>\n",
       "      <td>948</td>\n",
       "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
       "      <td>An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>T10</td>\n",
       "      <td>2.95973</td>\n",
       "      <td>1690</td>\n",
       "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
       "      <td>Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>T11</td>\n",
       "      <td>6.04910</td>\n",
       "      <td>987</td>\n",
       "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
       "      <td>Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>T12</td>\n",
       "      <td>6.18613</td>\n",
       "      <td>902</td>\n",
       "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
       "      <td>Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>T13</td>\n",
       "      <td>3.55312</td>\n",
       "      <td>1665</td>\n",
       "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
       "      <td>Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&amp;T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>T14</td>\n",
       "      <td>3.98397</td>\n",
       "      <td>250</td>\n",
       "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
       "      <td>266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>T15</td>\n",
       "      <td>5.13757</td>\n",
       "      <td>679</td>\n",
       "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
       "      <td>A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>T16</td>\n",
       "      <td>2.71369</td>\n",
       "      <td>1124</td>\n",
       "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
       "      <td>Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>T17</td>\n",
       "      <td>5.55943</td>\n",
       "      <td>1102</td>\n",
       "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
       "      <td>A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>T18</td>\n",
       "      <td>5.65911</td>\n",
       "      <td>537</td>\n",
       "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
       "      <td>Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>T19</td>\n",
       "      <td>3.73027</td>\n",
       "      <td>911</td>\n",
       "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
       "      <td>Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>T20</td>\n",
       "      <td>6.01090</td>\n",
       "      <td>72</td>\n",
       "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
       "      <td>73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topic  Max Score  Paper Num  \\\n",
       "Topic1              T1    1.64138        991   \n",
       "Topic2              T2    3.58149        383   \n",
       "Topic3              T3    5.83072       1167   \n",
       "Topic4              T4    3.93349       1731   \n",
       "Topic5              T5    2.98750         33   \n",
       "Topic6              T6    7.51003          5   \n",
       "Topic7              T7    4.89525       1318   \n",
       "Topic8              T8    3.67982        235   \n",
       "Topic9              T9    4.88831        948   \n",
       "Topic10            T10    2.95973       1690   \n",
       "Topic11            T11    6.04910        987   \n",
       "Topic12            T12    6.18613        902   \n",
       "Topic13            T13    3.55312       1665   \n",
       "Topic14            T14    3.98397        250   \n",
       "Topic15            T15    5.13757        679   \n",
       "Topic16            T16    2.71369       1124   \n",
       "Topic17            T17    5.55943       1102   \n",
       "Topic18            T18    5.65911        537   \n",
       "Topic19            T19    3.73027        911   \n",
       "Topic20            T20    6.01090         72   \n",
       "\n",
       "                                                                                                                                                                                                           Topic  \\\n",
       "Topic1                           bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "Topic2               neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
       "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
       "Topic4                                image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...   \n",
       "Topic6                                       cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property   \n",
       "Topic7              word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
       "Topic8                                       signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
       "Topic9                            control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant   \n",
       "Topic10                                 circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp   \n",
       "Topic11              spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution   \n",
       "Topic12                       rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned   \n",
       "Topic13                      node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation   \n",
       "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra   \n",
       "Topic15     classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian   \n",
       "Topic16   distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum   \n",
       "Topic17                                     motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed   \n",
       "Topic18                    object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint   \n",
       "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation   \n",
       "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol   \n",
       "\n",
       "                                                                                                                                                                                                      Paper Name  \n",
       "Topic1   A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&T Research \\nABSTRACT\\n1 INTRODUCTION...  \n",
       "Topic2   Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...  \n",
       "Topic3   Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...  \n",
       "Topic4   Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...  \n",
       "Topic5   5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...  \n",
       "Topic6   317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...  \n",
       "Topic7   Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...  \n",
       "Topic8   232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...  \n",
       "Topic9   An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...  \n",
       "Topic10  Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...  \n",
       "Topic11  Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...  \n",
       "Topic12  Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...  \n",
       "Topic13  Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...  \n",
       "Topic14  266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...  \n",
       "Topic15  A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...  \n",
       "Topic16  Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...  \n",
       "Topic17  A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...  \n",
       "Topic18  Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...  \n",
       "Topic19  Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...  \n",
       "Topic20  73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_score_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_score_topics.index\n",
    "term_score = max_score_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
    "                       for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Topics for New Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New Papers: 4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# papers manually downloaded from NIPS 16\n",
    "# https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016\n",
    "\n",
    "new_paper_files = glob.glob('./test_data/nips16*.txt')\n",
    "new_papers = []\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "              \n",
    "print('Total New Papers:', len(new_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14408)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_new_papers = normalize_corpus(new_papers)\n",
    "cv_new_features = cv.transform(norm_new_papers)\n",
    "cv_new_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1.312), (7, 0.966)],\n",
       " [(2, 4.121), (0, 0.864)],\n",
       " [(3, 2.154), (1, 1.335)],\n",
       " [(3, 3.074), (6, 2.19)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_predictions = nmf_model.transform(cv_new_features)\n",
    "best_topics = [[(topic, round(sc, 3)) \n",
    "                    for topic, sc in sorted(enumerate(topic_predictions[i]), \n",
    "                                            key=lambda row: -row[1])[:2]] \n",
    "                        for i in range(len(topic_predictions))]\n",
    "best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topics</th>\n",
       "      <th>Topic Score</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper Desc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Papers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>131.20000</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>96.60000</td>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>412.10000</td>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>86.40000</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>215.40000</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>133.50000</td>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>307.40000</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>219.00000</td>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topics  Topic Score  \\\n",
       "Papers                                 \n",
       "1                     1    131.20000   \n",
       "1                     8     96.60000   \n",
       "2                     3    412.10000   \n",
       "2                     1     86.40000   \n",
       "3                     4    215.40000   \n",
       "3                     2    133.50000   \n",
       "4                     4    307.40000   \n",
       "4                     7    219.00000   \n",
       "\n",
       "                                                                                                                                                                                                     Topic Desc  \\\n",
       "Papers                                                                                                                                                                                                            \n",
       "1                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "1                                           signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
       "2       state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
       "2                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "3                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "3                   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
       "4                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "4                  word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
       "\n",
       "                                                                                                                                                                                                     Paper Desc  \n",
       "Papers                                                                                                                                                                                                           \n",
       "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
       "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
       "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
       "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
       "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  \n",
       "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, sc in item] for item in best_topics]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Topic Score'] = [topic_sc for topic_list in \n",
    "                                        [[round(sc*100, 2) \n",
    "                                              for topic_num, sc in item] \n",
    "                                                 for item in best_topics] \n",
    "                                    for topic_sc in topic_list]\n",
    "\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Model and Transformers\n",
    "\n",
    "### This is just for visualizing the topics in the other notebook (since PyLDAViz expands the notebook size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('nmf_model.pkl', 'wb') as f:\n",
    "    dill.dump(nmf_model, f)\n",
    "with open('cv_features.pkl', 'wb') as f:\n",
    "    dill.dump(cv_features, f)\n",
    "with open('cv.pkl', 'wb') as f:\n",
    "    dill.dump(cv, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
